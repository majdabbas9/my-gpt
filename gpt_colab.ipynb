{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# GPT from Scratch: Training on 'The Verdict'\n",
                "This notebook provides a complete environment for training a GPT-style model from scratch. It consolidates all modular components (Attention, Transformer Blocks, Dataset Loading) into a single file optimized for Google Colab GPUs."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Data Download"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install tiktoken torch matplotlib numpy\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import tiktoken\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "import os\n",
                "import urllib.request\n",
                "\n",
                "# Download the data\n",
                "file_path = \"the-verdict.txt\"\n",
                "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
                "if not os.path.exists(file_path):\n",
                "    print(\"Downloading training data...\")\n",
                "    with urllib.request.urlopen(url) as response:\n",
                "        text_data = response.read().decode('utf-8')\n",
                "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
                "        f.write(text_data)\n",
                "else:\n",
                "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
                "        text_data = f.read()\n",
                "\n",
                "print(f\"Data loaded. Total characters: {len(text_data)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Model Architecture\n",
                "This section implements the LayerNorm, GELU, FeedForward, Multi-Head Attention, and the GPT Model itself."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LayerNorm(nn.Module):\n",
                "    def __init__(self, emb_dim):\n",
                "        super().__init__()\n",
                "        self.eps = 1e-5\n",
                "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
                "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
                "    def forward(self, x):\n",
                "        mean = x.mean(dim=-1, keepdim=True)\n",
                "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
                "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
                "        return self.scale * norm_x + self.shift\n",
                "\n",
                "class GELU(nn.Module):\n",
                "    def forward(self, x):\n",
                "        return 0.5 * x * (1 + torch.tanh(\n",
                "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
                "            (x + 0.044715 * torch.pow(x, 3))\n",
                "        ))\n",
                "\n",
                "class FeedForward(nn.Module):\n",
                "    def __init__(self, cfg):\n",
                "        super().__init__()\n",
                "        self.layers = nn.Sequential(\n",
                "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
                "            GELU(),\n",
                "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
                "        )\n",
                "    def forward(self, x):\n",
                "        return self.layers(x)\n",
                "\n",
                "class MultiHeadAttention(nn.Module):\n",
                "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
                "        super().__init__()\n",
                "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
                "        self.d_out = d_out\n",
                "        self.num_heads = num_heads\n",
                "        self.head_dim = d_out // num_heads\n",
                "        self.W_qkv = nn.Linear(d_in, d_out * 3, bias=qkv_bias)\n",
                "        self.out_proj = nn.Linear(d_out, d_out)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "        self.register_buffer(\n",
                "            'mask',\n",
                "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        batch_size, num_tokens, d_in = x.shape\n",
                "        qkv = self.W_qkv(x)\n",
                "        qkv = qkv.view(batch_size, num_tokens, 3, self.num_heads, self.head_dim)\n",
                "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
                "        queries, keys, values = qkv.unbind(0)\n",
                "        attn_scores = queries @ keys.transpose(-2, -1)\n",
                "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
                "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
                "        attn_weights = self.dropout(attn_weights)\n",
                "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
                "        context_vec = context_vec.contiguous().view(batch_size, num_tokens, self.d_out)\n",
                "        context_vec = self.out_proj(context_vec)\n",
                "        return context_vec\n",
                "\n",
                "class TransformerBlock(nn.Module):\n",
                "    def __init__(self, cfg):\n",
                "        super().__init__()\n",
                "        self.att = MultiHeadAttention(\n",
                "            d_in=cfg[\"emb_dim\"], d_out=cfg[\"emb_dim\"],\n",
                "            context_length=cfg[\"context_length\"], num_heads=cfg[\"n_heads\"],\n",
                "            dropout=cfg[\"drop_rate\"], qkv_bias=cfg[\"qkv_bias\"])\n",
                "        self.ff = FeedForward(cfg)\n",
                "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
                "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
                "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
                "\n",
                "    def forward(self, x):\n",
                "        shortcut = x\n",
                "        x = self.norm1(x)\n",
                "        x = self.att(x)\n",
                "        x = self.drop_shortcut(x)\n",
                "        x = x + shortcut\n",
                "\n",
                "        shortcut = x\n",
                "        x = self.norm2(x)\n",
                "        x = self.ff(x)\n",
                "        x = self.drop_shortcut(x)\n",
                "        x = x + shortcut\n",
                "        return x\n",
                "\n",
                "class GPTModel(nn.Module):\n",
                "    def __init__(self, cfg):\n",
                "        super().__init__()\n",
                "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
                "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
                "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
                "        self.trf_blocks = nn.Sequential(\n",
                "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
                "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
                "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
                "\n",
                "    def forward(self, in_idx):\n",
                "        batch_size, seq_len = in_idx.shape\n",
                "        tok_embeddings = self.tok_emb(in_idx)\n",
                "        pos_embeddings = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
                "        x = tok_embeddings + pos_embeddings\n",
                "        x = self.drop_emb(x)\n",
                "        x = self.trf_blocks(x)\n",
                "        x = self.final_norm(x)\n",
                "        return self.out_head(x)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Loading and Helpers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
                "\n",
                "def text_to_token_ids(text, tokenizer):\n",
                "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
                "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
                "    return encoded_tensor\n",
                "\n",
                "def token_ids_to_text(token_ids, tokenizer):\n",
                "    flat = token_ids.squeeze(0)\n",
                "    return tokenizer.decode(flat.tolist())\n",
                "\n",
                "class GPTDatasetV1(Dataset):\n",
                "    def __init__(self, txt, max_length, stride):\n",
                "        self.input_ids = []\n",
                "        self.target_ids = []\n",
                "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
                "        for i in range(0, len(token_ids) - max_length, stride):\n",
                "            input_chunk = token_ids[i:i + max_length]\n",
                "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
                "            self.input_ids.append(torch.tensor(input_chunk))\n",
                "            self.target_ids.append(torch.tensor(target_chunk))\n",
                "    def __len__(self):\n",
                "        return len(self.input_ids)\n",
                "    def __getitem__(self, idx):\n",
                "        return self.input_ids[idx], self.target_ids[idx]\n",
                "\n",
                "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
                "    dataset = GPTDatasetV1(txt, max_length, stride)\n",
                "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
                "    return dataloader"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Training Utilities"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def calc_loss_batch(input_batch, target_batch, model, device):\n",
                "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
                "    logits = model(input_batch)\n",
                "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
                "    return loss\n",
                "\n",
                "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
                "    total_loss = 0.\n",
                "    if len(data_loader) == 0: return float(\"nan\")\n",
                "    elif num_batches is None: num_batches = len(data_loader)\n",
                "    else: num_batches = min(num_batches, len(data_loader))\n",
                "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
                "        if i < num_batches:\n",
                "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
                "            total_loss += loss.item()\n",
                "        else: break\n",
                "    return total_loss / num_batches\n",
                "\n",
                "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
                "    model.eval()\n",
                "    with torch.no_grad():\n",
                "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
                "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
                "    model.train()\n",
                "    return train_loss, val_loss\n",
                "\n",
                "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
                "    for _ in range(max_new_tokens):\n",
                "        idx_cond = idx[:, -context_size:]\n",
                "        with torch.no_grad():\n",
                "            logits = model(idx_cond)\n",
                "        logits = logits[:, -1, :]\n",
                "        probas = torch.softmax(logits, dim=-1)\n",
                "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
                "        idx = torch.cat((idx, idx_next), dim=1)\n",
                "    return idx\n",
                "\n",
                "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
                "    model.eval()\n",
                "    context_size = model.pos_emb.weight.shape[0]\n",
                "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
                "    with torch.no_grad():\n",
                "        token_ids = generate_text_simple(model=model, idx=encoded, max_new_tokens=50, context_size=context_size)\n",
                "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
                "        print(decoded_text.replace(\"\\n\", \" \"))\n",
                "    model.train()\n",
                "\n",
                "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer):\n",
                "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
                "    tokens_seen, global_step = 0, -1\n",
                "    for epoch in range(num_epochs):\n",
                "        model.train()\n",
                "        for input_batch, target_batch in train_loader:\n",
                "            optimizer.zero_grad()\n",
                "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "            tokens_seen += input_batch.numel()\n",
                "            global_step += 1\n",
                "            if global_step % eval_freq == 0:\n",
                "                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
                "                train_losses.append(train_loss)\n",
                "                val_losses.append(val_loss)\n",
                "                track_tokens_seen.append(tokens_seen)\n",
                "                print(f\"Ep {epoch+1} (Step {global_step:06d}): Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
                "                generate_and_print_sample(model, tokenizer, device, start_context)\n",
                "    return train_losses, val_losses, track_tokens_seen"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Main Execution and Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "GPT_CONFIG_124M = {\n",
                "    \"vocab_size\": 50257,    \"context_length\": 256,\n",
                "    \"emb_dim\": 768,         \"n_heads\": 12,\n",
                "    \"n_layers\": 12,         \"drop_rate\": 0.1, \"qkv_bias\": False\n",
                "}\n",
                "\n",
                "torch.manual_seed(123)\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "train_ratio = 0.90\n",
                "split_idx = int(train_ratio * len(text_data))\n",
                "train_loader = create_dataloader_v1(text_data[:split_idx], batch_size=2, max_length=GPT_CONFIG_124M[\"context_length\"], stride=GPT_CONFIG_124M[\"context_length\"], drop_last=True, shuffle=True)\n",
                "val_loader = create_dataloader_v1(text_data[split_idx:], batch_size=2, max_length=GPT_CONFIG_124M[\"context_length\"], stride=GPT_CONFIG_124M[\"context_length\"], drop_last=False, shuffle=False)\n",
                "\n",
                "model = GPTModel(GPT_CONFIG_124M).to(device)\n",
                "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
                "\n",
                "train_losses, val_losses, tokens_seen = train_model_simple(\n",
                "    model, train_loader, val_loader, optimizer, device,\n",
                "    num_epochs=10, eval_freq=5, eval_iter=1, \n",
                "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Plotting Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
                "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
                "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
                "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
                "    ax1.set_xlabel(\"Epochs\")\n",
                "    ax1.set_ylabel(\"Loss\")\n",
                "    ax1.legend(loc=\"upper right\")\n",
                "    ax2 = ax1.twiny()\n",
                "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
                "    ax2.set_xlabel(\"Tokens seen\")\n",
                "    fig.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "epochs_tensor = torch.linspace(0, 10, len(train_losses))\n",
                "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Save and Test Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "torch.save(model.state_dict(), \"model_final.pth\")\n",
                "print(\"Model saved to model_final.pth\")\n",
                "generate_and_print_sample(model, tokenizer, device, \"In the end, it was\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Test the Saved Model \n",
                "This section verifies that the model was saved correctly by loading it into a fresh instance and generating text."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Create a new model instance with the same configuration\n",
                "loaded_model = GPTModel(GPT_CONFIG_124M).to(device)\n",
                "\n",
                "# 2. Load the saved weights\n",
                "loaded_model.load_state_dict(torch.load(\"model_final.pth\"))\n",
                "loaded_model.eval()\n",
                "\n",
                "print(\"Model loaded successfully from model_final.pth\")\n",
                "\n",
                "# 3. Run a test generation\n",
                "test_context = \"The future of technology\"\n",
                "print(f\"\\nTesting generation with context: {test_context}\")\n",
                "generate_and_print_sample(loaded_model, tokenizer, device, test_context)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}